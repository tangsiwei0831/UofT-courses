{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CSC420A2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPNQYSwIU_2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b444be-53ef-4175-a8dd-73f34a46bf6f"
      },
      "source": [
        "# part of the code is from the Tutorial 6, and for preprocessing part, since I write with teammate Yiyun Ding, so part of the code is similar.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGWlerK515qF"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "from torchsummary import summary\n",
        "import sys\n",
        "import tarfile\n",
        "import cv2\n",
        "from IPython.display import display, Image\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from six.moves import range\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import seaborn as sns\n",
        "# PyTorch\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torch import optim, cuda\n",
        "from torch.utils.data import DataLoader, sampler, TensorDataset\n",
        "import torch.nn as nn\n",
        "from timeit import default_timer as timer\n",
        "from PIL import UnidentifiedImageError, Image\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoD1FijjsHgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230cb52f-1e93-4871-dcb1-7533451750d7"
      },
      "source": [
        "# extract tar.gz fromat file from google drive\n",
        "def extract(filename, force=False):\n",
        "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
        "  tar = tarfile.open('/content/drive/My Drive/'+ filename)\n",
        "  sys.stdout.flush()\n",
        "  tar.extractall()\n",
        "  tar.close()\n",
        "  data_folders = [\n",
        "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
        "    if os.path.isdir(os.path.join(root, d))]\n",
        "  print(data_folders)\n",
        "  return data_folders\n",
        "\n",
        "data_filename = 'notMNIST_small.tar.gz'\n",
        "data_folders = extract(data_filename) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Xjow68tuq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b8bd35-a4d5-4271-af74-d1fb7be3fce1"
      },
      "source": [
        "mainfolders = ['test', 'train', 'valid']\n",
        "subfolders = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
        "path = \"/content/notMNIST_small/\"\n",
        "# create folders and subfolders for training, validation and testing\n",
        "for i in mainfolders:\n",
        "  try:\n",
        "      os.mkdir(path + i)\n",
        "  except:\n",
        "      print(path + i)\n",
        "for i in mainfolders:\n",
        "  for j in subfolders:\n",
        "    try: \n",
        "      os.mkdir(path + i + '/' + j)\n",
        "    except:\n",
        "      print( path + i + '/' + j)\n",
        "# divide images into the three created folders and each folder has ten subfolders\n",
        "for subfolder in subfolders:\n",
        "  image_files = os.listdir('notMNIST_small/' + subfolder)\n",
        "  i = 0\n",
        "  # index 0 -1499 images into training set\n",
        "  for i in range(1500):\n",
        "      shutil.copyfile('notMNIST_small/{}/{}'.format(subfolder, image_files[i]), \n",
        "                      'notMNIST_small/train/{}/{}'.format(subfolder, image_files[i]))\n",
        "  # index 1500 -1599 images into validation set\n",
        "  for i in range(1500, 1600):\n",
        "    shutil.copyfile('notMNIST_small/{}/{}'.format(subfolder, image_files[i]), \n",
        "                      'notMNIST_small/valid/{}/{}'.format(subfolder, image_files[i]))\n",
        "  # put rest images into test set\n",
        "  for i in range(1600, len(image_files)):\n",
        "    shutil.copyfile('notMNIST_small/{}/{}'.format(subfolder, image_files[i]), \n",
        "                      'notMNIST_small/test/{}/{}'.format(subfolder, image_files[i]))\n",
        "# for images cannot load, remove the images\n",
        "for route in mainfolders:\n",
        "  for subfolder in subfolders:\n",
        "    subfolder_path = 'notMNIST_small/' + route + '/' + subfolder\n",
        "    image_files = os.listdir(subfolder_path)\n",
        "    for image in image_files:\n",
        "      image_path = subfolder_path + '/' + image\n",
        "      try:\n",
        "        im = Image.open(image_path)\n",
        "        im.load()\n",
        "      except UnidentifiedImageError:\n",
        "        os.remove(image_path)\n",
        "        print(f'remove {image_path}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove notMNIST_small/test/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png\n",
            "remove notMNIST_small/train/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpf-45-tDcxc"
      },
      "source": [
        "training_dir = \"/content/notMNIST_small/train\"\n",
        "validation_dir = \"/content/notMNIST_small/test\"\n",
        "test_dir = \"/content/notMNIST_small/valid\"\n",
        "batch_size = 128\n",
        "data = {\n",
        "    'train':\n",
        "        datasets.ImageFolder(\n",
        "            root=training_dir, \n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), \n",
        "                transforms.Lambda(lambda x: x[0, :, :].reshape(784))\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "    'val':\n",
        "        datasets.ImageFolder(\n",
        "            root=validation_dir, \n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), \n",
        "                transforms.Lambda(lambda x: x[0, :, :].reshape(784))\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "    'test':\n",
        "        datasets.ImageFolder(\n",
        "            root=test_dir, \n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), \n",
        "                transforms.Lambda(lambda x: x[0, :, :].reshape(784))\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size = batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size = batch_size, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size = batch_size, shuffle=True)\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2FdEy1uLI-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e3ac9e-224b-443e-f8e4-b4cd892fa2dd"
      },
      "source": [
        "input_size = 784\n",
        "hidden_sizes = 1000\n",
        "output_size = 10\n",
        "#build model\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes),\n",
        "                      # uncomment for dropout\n",
        "                      # nn.Dropout(0.5),\n",
        "                      nn.ReLU(),\n",
        "                      # uncomment next two line for second layer and divide hidden size by 2\n",
        "                      # nn.Linear(hidden_sizes, hidden_sizes),\n",
        "                      # nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes, output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  (3): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCggj_ar3bFi",
        "outputId": "93dbeefd-c16e-4062-ec0a-7102a17a061a"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0002)\n",
        "summary(\n",
        "        model, input_size=(3, 784), batch_size=128, device='cuda')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1             [128, 3, 1000]         785,000\n",
            "              ReLU-2             [128, 3, 1000]               0\n",
            "            Linear-3               [128, 3, 10]          10,010\n",
            "           Softmax-4               [128, 3, 10]               0\n",
            "================================================================\n",
            "Total params: 795,010\n",
            "Trainable params: 795,010\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.15\n",
            "Forward/backward pass size (MB): 5.92\n",
            "Params size (MB): 3.03\n",
            "Estimated Total Size (MB): 10.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUGURnVIEflE"
      },
      "source": [
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          test_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20,\n",
        "          print_every=2):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        test_loader (PyTorch dataloader): test dataloader for testing accuracy\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "        test_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(data)\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Test loop\n",
        "                for data, target in test_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Test loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    test_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate test accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    test_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "                test_loss = test_loss / len(test_loader.dataset)\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "                test_acc = test_acc / len(test_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, test_loss, train_acc, valid_acc, test_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f}% \\t Validation Loss: {valid_loss:.4f}% \\t Test Loss: {test_loss:.4f}%'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}% \\t Validation Accuracy: {100 * valid_acc:.2f}% \\t Test Accuracy: {100 * test_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'test_loss', 'train_acc',\n",
        "                                'valid_acc', 'test_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'test_loss', 'train_acc', 'valid_acc', 'test_acc'])\n",
        "    return model, history\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y9F6Q7WHf8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "0766f309-43e7-4efc-d14f-70c68d62c58f"
      },
      "source": [
        "save_file_name = 'onelayer-finetuned.pt'\n",
        "train_on_gpu = False\n",
        "model, history = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    dataloaders['test'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=100,\n",
        "    print_every=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training from Scratch.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c59dcc953c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_epochs_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     print_every=2)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-fca34292a907>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, test_loader, save_file_name, max_epochs_stop, n_epochs, print_every)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Predicted outputs are log probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Loss and backpropagation of gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10752x28 and 784x1000)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbYAAQI1gxmF"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc','valid_acc','test_acc']:\n",
        "    plt.plot(\n",
        "        100 * (1-history[c]), label= f'{c[:-4]}_error')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Training, Validation and Test error')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}