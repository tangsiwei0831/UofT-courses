{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC420A2-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t4HYhZgjqsJ",
        "outputId": "69f8bc4b-a1dd-4501-8b0f-ab5e0edc1e30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdhLo_IAjvXD"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "from scipy import ndimage\n",
        "import cv2\n",
        "from IPython.display import display, Image\n",
        "from scipy import misc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from six.moves import cPickle as pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEu4O9G5lctf",
        "outputId": "50680cee-2a00-4f49-8d7a-946625f93adb"
      },
      "source": [
        "def maybe_extract(filename, force=False):\n",
        "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
        "  if os.path.isdir(root) and not force:\n",
        "    # You may override by setting force=True.\n",
        "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
        "  else:\n",
        "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
        "    tar = tarfile.open('/content/drive/My Drive/'+filename)\n",
        "    sys.stdout.flush()\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "  data_folders = [\n",
        "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
        "    if os.path.isdir(os.path.join(root, d))]\n",
        "  if len(data_folders) != 10:\n",
        "    raise Exception(\n",
        "      'Expected %d folders, one per class. Found %d instead.' % (\n",
        "        num_classes, len(data_folders)))\n",
        "  print(data_folders)\n",
        "  return data_folders\n",
        "\n",
        "test_filename = 'notMNIST_small.tar.gz'\n",
        "data_folders = maybe_extract(test_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notMNIST_small already present - Skipping extraction of notMNIST_small.tar.gz.\n",
            "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu-ADg_1lfEd",
        "outputId": "6030efc0-4d63-4126-a7c2-289a1d5cb50d"
      },
      "source": [
        "# image_size = 28  # Pixel width and height.\n",
        "# pixel_depth = 255.0  # Number of levels per pixel.\n",
        "\n",
        "# def load_letter(folder, start, min_num_images):\n",
        "#   \"\"\"Load the data for a single letter label.\"\"\"\n",
        "#   image_files = os.listdir(folder)\n",
        "#   dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
        "#                          dtype=np.float32)\n",
        "#   image_index = 0\n",
        "#   print(folder)\n",
        "#   for image in os.listdir(folder):\n",
        "#     image_file = os.path.join(folder, image)\n",
        "#     try:\n",
        "#       # image_data = (ndimage.imread(image_file).astype(float) - \n",
        "#       #               pixel_depth / 2) / pixel_depth\n",
        "#       image_read = cv2.imread(image_file)\n",
        "#       if image_read is not None:\n",
        "#         image_data = (image_read.astype(float) - \n",
        "#                       pixel_depth / 2) / pixel_depth\n",
        "#       if image_data.shape != (image_size, image_size, 3):\n",
        "#         raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
        "#       dataset[image_index, :, :] = image_data[:, :, 0]\n",
        "#       image_index += 1\n",
        "#     except IOError as e:\n",
        "#       print('Could not read file - it\\'s ok, skipping.')\n",
        "#   dataset = dataset[start:min_num_images, :, :]\n",
        "#   return dataset\n",
        "        \n",
        "# def maybe_pickle(data_folders,start, min_num_images_per_class, force=False):\n",
        "#   dataset_names = []\n",
        "#   for folder in data_folders:\n",
        "#     set_filename = folder + str(start) + '.pickle'\n",
        "#     dataset_names.append(set_filename)\n",
        "#     if os.path.exists(set_filename) and not force:\n",
        "#       # You may override by setting force=True.\n",
        "#       print('%s already present - Skipping pickling.' % set_filename)\n",
        "#     else:\n",
        "#       print('Pickling %s.' % set_filename)\n",
        "#       dataset = load_letter(folder,start, min_num_images_per_class)\n",
        "#       try:\n",
        "#         with open(set_filename, 'wb') as f:\n",
        "#           pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
        "#       except Exception as e:\n",
        "#         print('Unable to save data to', set_filename, ':', e)\n",
        "  \n",
        "#   return dataset_names\n",
        "\n",
        "# train_datasets = maybe_pickle(data_folders,0, 1500)\n",
        "# validation_datasets = maybe_pickle(data_folders,1500, 1600)\n",
        "# test_datasets = maybe_pickle(data_folders,1600, 1874)\n",
        "image_size = 28  # Pixel width and height.\n",
        "pixel_depth = 255.0  # Number of levels per pixel.\n",
        "\n",
        "def load_letter(folder, min_num_images):\n",
        "  \"\"\"Load the data for a single letter label.\"\"\"\n",
        "  image_files = os.listdir(folder)\n",
        "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
        "                         dtype=np.float32)\n",
        "  image_index = 0\n",
        "  print(folder)\n",
        "  for image in os.listdir(folder):\n",
        "    image_file = os.path.join(folder, image)\n",
        "    try:\n",
        "      image_read = cv2.imread(image_file)\n",
        "      if image_read is not None:\n",
        "        image_data = (image_read.astype(float) - \n",
        "                      pixel_depth / 2) / pixel_depth\n",
        "      if image_data.shape != (image_size, image_size, 3):\n",
        "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
        "      dataset[image_index, :, :] = image_data[:, :, 0]\n",
        "      image_index += 1\n",
        "    except IOError as e:\n",
        "      print('Could not read file - it\\'s ok, skipping.')\n",
        "    \n",
        "  num_images = image_index\n",
        "  dataset = dataset[0:num_images, :, :]\n",
        "  if num_images < min_num_images:\n",
        "    raise Exception('Many fewer images than expected: %d < %d' %\n",
        "                    (num_images, min_num_images))\n",
        "    \n",
        "  print('Full dataset tensor:', dataset.shape)\n",
        "  print('Mean:', np.mean(dataset))\n",
        "  print('Standard deviation:', np.std(dataset))\n",
        "  return dataset\n",
        "        \n",
        "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
        "  dataset_names = []\n",
        "  for folder in data_folders:\n",
        "    set_filename = folder + '.pickle'\n",
        "    dataset_names.append(set_filename)\n",
        "    if os.path.exists(set_filename) and not force:\n",
        "      # You may override by setting force=True.\n",
        "      print('%s already present - Skipping pickling.' % set_filename)\n",
        "    else:\n",
        "      print('Pickling %s.' % set_filename)\n",
        "      dataset = load_letter(folder, min_num_images_per_class)\n",
        "      try:\n",
        "        with open(set_filename, 'wb') as f:\n",
        "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
        "      except Exception as e:\n",
        "        print('Unable to save data to', set_filename, ':', e)\n",
        "  \n",
        "  return dataset_names\n",
        "\n",
        "train_datasets = maybe_pickle(data_folders, 1500)\n",
        "validation_datasets = maybe_pickle(data_folders, 100)\n",
        "test_datasets = maybe_pickle(data_folders, 272)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notMNIST_small/A.pickle already present - Skipping pickling.\n",
            "notMNIST_small/B.pickle already present - Skipping pickling.\n",
            "notMNIST_small/C.pickle already present - Skipping pickling.\n",
            "notMNIST_small/D.pickle already present - Skipping pickling.\n",
            "notMNIST_small/E.pickle already present - Skipping pickling.\n",
            "notMNIST_small/F.pickle already present - Skipping pickling.\n",
            "notMNIST_small/G.pickle already present - Skipping pickling.\n",
            "notMNIST_small/H.pickle already present - Skipping pickling.\n",
            "notMNIST_small/I.pickle already present - Skipping pickling.\n",
            "notMNIST_small/J.pickle already present - Skipping pickling.\n",
            "notMNIST_small/A.pickle already present - Skipping pickling.\n",
            "notMNIST_small/B.pickle already present - Skipping pickling.\n",
            "notMNIST_small/C.pickle already present - Skipping pickling.\n",
            "notMNIST_small/D.pickle already present - Skipping pickling.\n",
            "notMNIST_small/E.pickle already present - Skipping pickling.\n",
            "notMNIST_small/F.pickle already present - Skipping pickling.\n",
            "notMNIST_small/G.pickle already present - Skipping pickling.\n",
            "notMNIST_small/H.pickle already present - Skipping pickling.\n",
            "notMNIST_small/I.pickle already present - Skipping pickling.\n",
            "notMNIST_small/J.pickle already present - Skipping pickling.\n",
            "notMNIST_small/A.pickle already present - Skipping pickling.\n",
            "notMNIST_small/B.pickle already present - Skipping pickling.\n",
            "notMNIST_small/C.pickle already present - Skipping pickling.\n",
            "notMNIST_small/D.pickle already present - Skipping pickling.\n",
            "notMNIST_small/E.pickle already present - Skipping pickling.\n",
            "notMNIST_small/F.pickle already present - Skipping pickling.\n",
            "notMNIST_small/G.pickle already present - Skipping pickling.\n",
            "notMNIST_small/H.pickle already present - Skipping pickling.\n",
            "notMNIST_small/I.pickle already present - Skipping pickling.\n",
            "notMNIST_small/J.pickle already present - Skipping pickling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxNpmxjAEEjo",
        "outputId": "9f750905-f447-4707-a1b9-c506f5b9ab1b"
      },
      "source": [
        "# def make_arrays(nb_rows, img_size):\n",
        "#   if nb_rows:\n",
        "#     dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
        "#     labels = np.ndarray(nb_rows, dtype=np.int32)\n",
        "#   else:\n",
        "#     dataset, labels = None, None\n",
        "#   return dataset, labels\n",
        "\n",
        "# def merge_datasets(pickle_files, train_size):\n",
        "#   num_classes = len(pickle_files)\n",
        "#   train_dataset, train_labels = make_arrays(train_size, image_size)\n",
        "#   tsize_per_class = train_size // num_classes\n",
        "    \n",
        "#   start_t = 0\n",
        "#   for label, pickle_file in enumerate(pickle_files):       \n",
        "#     try:\n",
        "#       with open(pickle_file, 'rb') as f:\n",
        "#         np.random.shuffle(letter_set)\n",
        "#         letter_set = pickle.load(f)\n",
        "#         train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
        "#         train_dataset[start_t:end_t, :, :] = train_letter\n",
        "#         train_labels[start_t:end_t] = label\n",
        "#         start_t += tsize_per_class\n",
        "#         end_t += tsize_per_class\n",
        "#     except Exception as e:\n",
        "#       print('Unable to process data from', pickle_file, ':', e)\n",
        "#       raise\n",
        "    \n",
        "#   return train_dataset, train_labels\n",
        "            \n",
        "            \n",
        "# train_size = 15000\n",
        "# valid_size = 1000\n",
        "# test_size = 2726\n",
        "\n",
        "# train_dataset, train_labels = merge_datasets(\n",
        "#   train_datasets, train_size)\n",
        "# test_dataset, test_labels = merge_datasets(\n",
        "#   test_datasets, test_size)\n",
        "# valid_dataset, valid_labels = merge_datasets(\n",
        "#   validation_datasets, valid_size)\n",
        "# print('Training:', train_dataset.shape, train_labels.shape)\n",
        "# print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
        "# print('Testing:', test_dataset.shape, test_labels.shape)\n",
        "# print(valid_dataset)\n",
        "#############################################################################################\n",
        "def load_letter_based_on_type(folder, dataset_type):\n",
        "  image_files = os.listdir(folder)\n",
        "\n",
        "  if dataset_type == 'train':\n",
        "    start_index = 0\n",
        "    end_index = 1500\n",
        "\n",
        "  elif dataset_type == 'validate':\n",
        "    start_index = 1500\n",
        "    end_index = 1600\n",
        "  elif dataset_type == 'test':\n",
        "    start_index = 1600\n",
        "    end_index = len(image_files)\n",
        "\n",
        "  image_index = 0\n",
        "  dataset = np.ndarray(shape=(end_index-start_index, 28, 28),\n",
        "                         dtype=np.float32)\n",
        "  for image in os.listdir(folder)[start_index:end_index]:\n",
        "    image_file = os.path.join(folder, image)\n",
        "    image_read = cv2.imread(image_file)\n",
        "    if image_read is not None:\n",
        "      image_data = (image_read.astype(float) - 255 / 2) / 255\n",
        "      dataset[image_index, :, :] = image_data[:, :, 0]\n",
        "    \n",
        "    image_index += 1\n",
        "    \n",
        "  return dataset\n",
        "\n",
        "def separate_folder(data_folders):\n",
        "  trainingset_names = []\n",
        "  testingset_names = []\n",
        "  validationset_names = []\n",
        "  for folder in data_folders:\n",
        "    training_folder = folder + '.training'\n",
        "    testing_folder = folder + '.testing'\n",
        "    validation_folder = folder + '.validation'\n",
        "\n",
        "    trainingset_names.append(training_folder)\n",
        "    testingset_names.append(testing_folder)\n",
        "    validationset_names.append(validation_folder)\n",
        "\n",
        "    trainingset = load_letter_based_on_type(folder, 'train')\n",
        "    testingset = load_letter_based_on_type(folder, 'test')\n",
        "    validationset = load_letter_based_on_type(folder, 'validate')\n",
        "\n",
        "    try:\n",
        "      with open(validation_folder, 'wb') as f:\n",
        "        pickle.dump(validationset, f, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "      print('Unable to save data to', validation_folder, ':', e)\n",
        "    \n",
        "    try:\n",
        "      with open(testing_folder, 'wb') as g:\n",
        "        pickle.dump(testingset, g, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "      print('Unable to save data to', testing_folder, ':', e)\n",
        "\n",
        "    try:\n",
        "      with open(training_folder, 'wb') as h:\n",
        "        pickle.dump(trainingset, h, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "      print('Unable to save data to', training_folder, ':', e)\n",
        "  return {'validate_name': validationset_names,\n",
        "          'training_name': trainingset_names,\n",
        "          'testing_name': testingset_names}\n",
        "\n",
        "folder_names_dictionary = separate_folder(data_folders)\n",
        "print(valid_dataset)\n",
        "######################################################################\n",
        "# def make_arrays(nb_rows, img_size):\n",
        "#   if nb_rows:\n",
        "#     dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
        "#     labels = np.ndarray(nb_rows, dtype=np.int32)\n",
        "#   else:\n",
        "#     dataset, labels = None, None\n",
        "#   return dataset, labels\n",
        "\n",
        "# def merge_datasets(pickle_files, train_size, valid_size=0):\n",
        "#   num_classes = len(pickle_files)\n",
        "#   valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
        "#   train_dataset, train_labels = make_arrays(train_size, image_size)\n",
        "#   vsize_per_class = valid_size // num_classes\n",
        "#   tsize_per_class = train_size // num_classes\n",
        "    \n",
        "#   start_v, start_t = 0, 0\n",
        "#   end_v, end_t = vsize_per_class, tsize_per_class\n",
        "#   end_l = vsize_per_class+tsize_per_class\n",
        "#   for label, pickle_file in enumerate(pickle_files):       \n",
        "#     try:\n",
        "#       with open(pickle_file, 'rb') as f:\n",
        "#         letter_set = pickle.load(f)\n",
        "#         # let's shuffle the letters to have random validation and training set\n",
        "#         np.random.shuffle(letter_set)\n",
        "#         if valid_dataset is not None:\n",
        "#           valid_letter = letter_set[:vsize_per_class, :, :]\n",
        "#           valid_dataset[start_v:end_v, :, :] = valid_letter\n",
        "#           valid_labels[start_v:end_v] = label\n",
        "#           start_v += vsize_per_class\n",
        "#           end_v += vsize_per_class\n",
        "                    \n",
        "#         train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
        "#         train_dataset[start_t:end_t, :, :] = train_letter\n",
        "#         train_labels[start_t:end_t] = label\n",
        "#         start_t += tsize_per_class\n",
        "#         end_t += tsize_per_class\n",
        "#     except Exception as e:\n",
        "#       print('Unable to process data from', pickle_file, ':', e)\n",
        "#       raise\n",
        "#   return valid_dataset, valid_labels, train_dataset, train_labels\n",
        "            \n",
        "            \n",
        "# train_size = 15000\n",
        "# valid_size = 1000\n",
        "# test_size = 2720\n",
        "\n",
        "# valid_dataset,valid_labels, train_dataset, train_labels = merge_datasets(\n",
        "#   train_datasets, train_size, valid_size)\n",
        "# _, _, valid_dataset, valid_labels = merge_datasets(validation_datasets, valid_size)\n",
        "# _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
        "\n",
        "# print('Training:', train_dataset.shape, train_labels.shape)\n",
        "# print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
        "# print('Testing:', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.5        -0.5        -0.5        ...  0.5         0.5\n",
            "   0.5       ]\n",
            " [-0.5        -0.5        -0.5        ...  0.11960784 -0.24901961\n",
            "  -0.5       ]\n",
            " [-0.5        -0.5        -0.5        ...  0.49607843  0.5\n",
            "   0.38627452]\n",
            " ...\n",
            " [-0.5        -0.5        -0.5        ... -0.5        -0.5\n",
            "  -0.5       ]\n",
            " [-0.5        -0.5        -0.5        ... -0.5        -0.5\n",
            "  -0.5       ]\n",
            " [-0.5        -0.5        -0.5        ... -0.5        -0.5\n",
            "  -0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy2y8jJoCrB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c8f0fa-c39d-4209-a7bc-47aa3d41b5de"
      },
      "source": [
        "def make_arrays(nb_rows, img_size):\n",
        "  if nb_rows:\n",
        "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
        "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
        "  else:\n",
        "    dataset, labels = None, None\n",
        "  return dataset, labels\n",
        "\n",
        "def merge(number, folder_names):\n",
        "  dataset, labels = make_arrays(number, 28)\n",
        "  start_index = 0\n",
        "  label = 0\n",
        "  for folder_name in folder_names:\n",
        "    with open(folder_name, 'rb') as file:\n",
        "      loaded_dataset = pickle.load(file)\n",
        "      dataset[start_index:start_index+len(loaded_dataset), :, :] = loaded_dataset\n",
        "      labels[start_index:start_index+len(loaded_dataset)] = label\n",
        "      start_index += len(loaded_dataset)\n",
        "      label += 1\n",
        "  return dataset, labels\n",
        "\n",
        "            \n",
        "valid_dataset, valid_labels = merge(1000, folder_names_dictionary['validate_name'])\n",
        "test_dataset, test_labels = merge(2726, folder_names_dictionary['testing_name'])\n",
        "train_dataset, train_labels = merge(15000, folder_names_dictionary['training_name'])\n",
        "\n",
        "print('Training:', train_dataset.shape, train_labels.shape)\n",
        "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
        "print('Testing:', test_dataset.shape, test_labels.shape)\n",
        "# print(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: (15000, 28, 28) (15000,)\n",
            "Validation: (1000, 28, 28) (1000,)\n",
            "Testing: (2726, 28, 28) (2726,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AEJHX5GuNrQ"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YGFWhDDt3pO",
        "outputId": "d34bbe87-0279-494f-833b-e291b33f11cf"
      },
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (15000, 784) (15000, 10)\n",
            "Validation set (1000, 784) (1000, 10)\n",
            "Test set (2726, 784) (2726, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpiwJK2w21o7"
      },
      "source": [
        "# Task II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfF5WhIp4R64"
      },
      "source": [
        "batch_size = 128\n",
        "hidden_nodes = 1000\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    #input data. For the training data, we use a placeholder that will be fed\n",
        "    #at run time with a training minibatch\n",
        "    tf_train_dataset = tf.compat.v1.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size*image_size), name=\"td\")\n",
        "    tf_train_labels = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, num_labels), name=\"tl\")\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    #variables\n",
        "    weights1 = tf.Variable(\n",
        "        tf.compat.v1.truncated_normal([image_size*image_size, hidden_nodes]))\n",
        "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
        "    weights2 =tf.Variable(\n",
        "        tf.compat.v1.truncated_normal([hidden_nodes, num_labels]))\n",
        "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "\n",
        "    #training computation.\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
        "    relu1 = tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu1, weights2) + biases2\n",
        "    # relu1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "    # relu_out= tf.nn.relu(tf.matmul(relu1, weights2) + biases2)\n",
        "\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(logits=logits_2,labels=tf_train_labels))\n",
        "\n",
        "    #optimizer\n",
        "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "    #predictions for the training, validation, and test data\n",
        "    train_prediction = tf.nn.softmax(logits_2)\n",
        "    valid_prediction = tf.nn.softmax(\n",
        "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1) +biases1), weights2) + biases2)\n",
        "    test_prediction = tf.nn.softmax(\n",
        "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LGLyjYF415l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218b074e-d097-4e04-f0dd-85c2fa976fdd"
      },
      "source": [
        "num_steps = 3001\n",
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])\n",
        "with tf.compat.v1.Session(graph=graph) as session:\n",
        "  tf.compat.v1.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "      print (\"============================\")\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 290.664185\n",
            "Minibatch accuracy: 7.0%\n",
            "Validation accuracy: 10.0%\n",
            "============================\n",
            "Minibatch loss at step 500: 197.119598\n",
            "Minibatch accuracy: 63.3%\n",
            "Validation accuracy: 42.0%\n",
            "============================\n",
            "Minibatch loss at step 1000: 144.398346\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 52.7%\n",
            "============================\n",
            "Minibatch loss at step 1500: 4.747934\n",
            "Minibatch accuracy: 55.5%\n",
            "Validation accuracy: 68.5%\n",
            "============================\n",
            "Minibatch loss at step 2000: 4.467845\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 76.3%\n",
            "============================\n",
            "Minibatch loss at step 2500: 0.547467\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 79.9%\n",
            "============================\n",
            "Minibatch loss at step 3000: 2.143633\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 82.3%\n",
            "============================\n",
            "Test accuracy: 80.9%\n"
          ]
        }
      ]
    }
  ]
}